"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9891],{9063:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"context-compaction","title":"Context Compaction","description":"Optimize AI conversations with automatic context management","source":"@site/docs/context-compaction.mdx","sourceDirName":".","slug":"/context-compaction","permalink":"/docs/context-compaction","draft":false,"unlisted":false,"editUrl":"https://github.com/antinomyhq/antinomyhq.github.io/tree/main/docs/context-compaction.mdx","tags":[],"version":"current","lastUpdatedAt":1743502566000,"sidebarPosition":12,"frontMatter":{"title":"Context Compaction","slug":"/context-compaction","sidebar_position":12,"description":"Optimize AI conversations with automatic context management","sidebar_label":"Context Compaction"},"sidebar":"docs","previous":{"title":"MCP Integration","permalink":"/docs/mcp-integration"},"next":{"title":"Custom Commands","permalink":"/docs/custom-commands"}}');var o=t(4848),s=t(8453);const r={title:"Context Compaction",slug:"/context-compaction",sidebar_position:12,description:"Optimize AI conversations with automatic context management",sidebar_label:"Context Compaction"},a="Context Compaction",l={},c=[{value:"What is Context Compaction?",id:"what-is-context-compaction",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Configuration Options",id:"configuration-options",level:2},{value:"Configuration Parameters",id:"configuration-parameters",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Selecting Appropriate Thresholds",id:"selecting-appropriate-thresholds",level:3},{value:"Choosing Summarization Models",id:"choosing-summarization-models",level:3},{value:"Retention Window Considerations",id:"retention-window-considerations",level:3},{value:"Example Use Cases",id:"example-use-cases",level:2},{value:"Long Debugging Sessions",id:"long-debugging-sessions",level:3},{value:"Multi-Stage Project Development",id:"multi-stage-project-development",level:3},{value:"Interactive Learning and Tutorials",id:"interactive-learning-and-tutorials",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue: Context Seems Lost After Compaction",id:"issue-context-seems-lost-after-compaction",level:3},{value:"Issue: Slow Responses After Threshold is Reached",id:"issue-slow-responses-after-threshold-is-reached",level:3},{value:"Related Features",id:"related-features",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"context-compaction",children:"Context Compaction"})}),"\n",(0,o.jsx)(n.p,{children:"Forge includes powerful automatic context management capabilities that optimize AI conversations while preserving important information."}),"\n",(0,o.jsx)(n.h2,{id:"what-is-context-compaction",children:"What is Context Compaction?"}),"\n",(0,o.jsx)(n.p,{children:"As conversations with AI agents grow longer, they can exceed token limits and become inefficient. Context compaction automatically summarizes older parts of conversations when they reach configurable thresholds, allowing you to maintain longer, more productive interactions without hitting model context limits."}),"\n",(0,o.jsx)(n.p,{children:"Key benefits include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Extended Conversations"}),": Continue conversations beyond normal token limits"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Optimized Performance"}),": Reduce token usage and improve response times"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Preserved Context"}),": Keep critical information while summarizing less important details"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost Efficiency"}),": Reduce token usage in API calls"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,o.jsx)(n.p,{children:"When a conversation reaches the configured token threshold:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"The system identifies which messages to preserve (based on the retention window)"}),"\n",(0,o.jsx)(n.li,{children:"Older messages are sent to the configured summarization model"}),"\n",(0,o.jsx)(n.li,{children:"A concise summary replaces the older messages"}),"\n",(0,o.jsx)(n.li,{children:"New conversation turns continue with the summarized context"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This process happens automatically and transparently to the user."}),"\n",(0,o.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,o.jsxs)(n.p,{children:["Add the following to your ",(0,o.jsx)(n.code,{children:"forge.yaml"})," file under an agent configuration:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'agents:\n  - id: assistant\n    model: anthropic/claude-3.5-sonnet\n    compact:\n      max_tokens: 2000 # Maximum tokens for the summary\n      token_threshold: 80000 # When to trigger compaction\n      model: google/gemini-2.0-flash-001 # Model to use for summarization\n      retention_window: 6 # Recent messages to preserve\n      prompt: "{{> system-prompt-context-summarizer.hbs }}" # Optional custom prompt\n'})}),"\n",(0,o.jsx)(n.h3,{id:"configuration-parameters",children:"Configuration Parameters"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Parameter"}),(0,o.jsx)(n.th,{children:"Required"}),(0,o.jsx)(n.th,{children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"max_tokens"})}),(0,o.jsx)(n.td,{children:"Yes"}),(0,o.jsx)(n.td,{children:"Maximum token count for the generated summary"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"token_threshold"})}),(0,o.jsx)(n.td,{children:"Yes"}),(0,o.jsx)(n.td,{children:"Conversation token count that triggers compaction"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"model"})}),(0,o.jsx)(n.td,{children:"Yes"}),(0,o.jsx)(n.td,{children:"AI model to use for generating the summary"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"retention_window"})}),(0,o.jsx)(n.td,{children:"No"}),(0,o.jsx)(n.td,{children:"Number of recent messages to preserve unchanged"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"prompt"})}),(0,o.jsx)(n.td,{children:"No"}),(0,o.jsx)(n.td,{children:"Custom prompt template for summarization"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"selecting-appropriate-thresholds",children:"Selecting Appropriate Thresholds"}),"\n",(0,o.jsxs)(n.p,{children:["Set ",(0,o.jsx)(n.code,{children:"token_threshold"})," based on your model's context window size. For example:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"For Claude 3.7 Sonnet (~200K token window): 150,000 to 180,000 tokens"}),"\n",(0,o.jsx)(n.li,{children:"For Claude 3.5 haiku (~200K token window): 120,000 to 160,000 tokens"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"choosing-summarization-models",children:"Choosing Summarization Models"}),"\n",(0,o.jsx)(n.p,{children:"For the summarization model, balance speed and quality:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Fast models (like Gemini Flash) provide quicker summaries"}),"\n",(0,o.jsx)(n.li,{children:"More powerful models may provide better context preservation but take longer"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"retention-window-considerations",children:"Retention Window Considerations"}),"\n",(0,o.jsx)(n.p,{children:"The retention window controls how many recent messages are preserved verbatim:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Larger windows maintain more recent context but reduce compaction efficiency"}),"\n",(0,o.jsx)(n.li,{children:"Smaller windows allow for more aggressive compaction but may lose recent details"}),"\n",(0,o.jsx)(n.li,{children:"A typical value is 6-10 messages"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The template receives the conversation history and should instruct the model on how to create an effective summary."}),"\n",(0,o.jsx)(n.h2,{id:"example-use-cases",children:"Example Use Cases"}),"\n",(0,o.jsx)(n.h3,{id:"long-debugging-sessions",children:"Long Debugging Sessions"}),"\n",(0,o.jsx)(n.p,{children:"When debugging complex issues, conversations can become lengthy. Context compaction allows the agent to remember key debugging steps while summarizing earlier diagnostics."}),"\n",(0,o.jsx)(n.h3,{id:"multi-stage-project-development",children:"Multi-Stage Project Development"}),"\n",(0,o.jsx)(n.p,{children:"For projects developed over multiple sessions, context compaction enables the agent to maintain awareness of project requirements and previous decisions while focusing on current tasks."}),"\n",(0,o.jsx)(n.h3,{id:"interactive-learning-and-tutorials",children:"Interactive Learning and Tutorials"}),"\n",(0,o.jsx)(n.p,{children:"When using Forge for learning or following tutorials, compaction helps maintain the thread of the lesson while summarizing earlier explanations."}),"\n",(0,o.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,o.jsx)(n.p,{children:"Context compaction runs asynchronously to minimize impact on response times. However, consider these performance factors:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Summarization Latency"}),": More powerful summarization models may take longer"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Summary Quality vs. Speed"}),": Balance between fast models and high-quality summaries"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Token Thresholds"}),": Lower thresholds trigger more frequent compaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Retention Window Size"}),": Larger windows preserve more context but reduce compaction efficiency"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"issue-context-seems-lost-after-compaction",children:"Issue: Context Seems Lost After Compaction"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Increase ",(0,o.jsx)(n.code,{children:"max_tokens"})," to allow for more detailed summaries"]}),"\n",(0,o.jsx)(n.li,{children:"Use a more capable summarization model"}),"\n",(0,o.jsx)(n.li,{children:"Increase the retention window to preserve more recent messages"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"issue-slow-responses-after-threshold-is-reached",children:"Issue: Slow Responses After Threshold is Reached"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Choose a faster summarization model"}),"\n",(0,o.jsx)(n.li,{children:"Reduce the token threshold to trigger earlier compaction"}),"\n",(0,o.jsxs)(n.li,{children:["Consider lowering ",(0,o.jsx)(n.code,{children:"max_tokens"})," if full context isn't critical"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"related-features",children:"Related Features"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../agent-configuration",children:"Agent Configuration"})," - Learn about other agent configuration options"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../operation-modes",children:"Operation Modes"})," - Understand how context works in different operation modes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../tools-reference",children:"Tools Reference"})," - Explore the tools agents use to interact with your system"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:"By effectively using context compaction, you can maintain longer, more productive AI conversations while optimizing for performance and cost efficiency."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);